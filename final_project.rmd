---
title: "CENG 4515 DATA SCIENCE AND ANALYTICS"
author: "Ayşe Ceren Çiçek & Gizem Kurnaz"
date: "01/13/2021"
output: 
  html_document: default
  pdf_document: default
  word_document: default
subtitle: Final Project
font-family: Gill Sans
---

**Dataset:** http://www2.informatik.uni-freiburg.de/~cziegler/BX/

The dataset contains three csv files which consists of ......

**Ratings.csv**: Contains the book rating information. Ratings (Book-Rating) are either explicit, expressed on a scale from 1-10 (higher values denoting higher appreciation), or implicit, expressed by 0.

- User.ID: User identification ID
- ISBN: ISBN Id for the book
- Book.Rating: Rating for each book given by user

**Users.csv**: Contains the users. User IDs (User-ID) have been anonymized and map to integers. Demographic data is provided (Location, Age) if available. Otherwise, these fields contain NULL-values

- User.ID: Unique identification ID for the user
- Location: Location of the user
- Age: Age of the user


**Books.csv**: Books are identified by their respective ISBN. Invalid ISBNs have already been removed from the dataset. Moreover, some content-based information is given (Book-Title, Book-Author, Year-Of-Publication, Publisher), obtained from Amazon Web Services. In case of several authors, only the first is provided. URLs linking to cover images are also given, appearing in three different flavours (Image-URL-S, Image-URL-M, Image-URL-L), i.e., small, medium, large. These URLs point to the Amazon web site.

- ISBN: ISBN ID for each book
- Book.Title: Title of the book
- Book.Author: Author name
- Year.Of.Publication: Publication year
- Publisher: Name of the publisher
- Image.URL.S: Small sized image URL of the book
- Image.URL.M: Medium sized image URL of the book
- Image.URL.L: Large sized image URL of the book

# Importing libraries

```{r, warning=FALSE,message=FALSE,error=FALSE, results='hide'}
library(funModeling) 
library(tidyverse) 
library(Hmisc)
library(dplyr)
library(data.table)
library(stringr)
library(ggplot2)
library(Hmisc)
library(missForest)
library(mice)
library(ROSE)
```


# Loading dataset

```{r, warning=FALSE}
ratings <- fread("BookDataset/ratings.csv", sep = ";")
head(ratings, n=5)
```

```{r,warning=FALSE}
users <- fread("BookDataset/users.csv", sep = ";")
head(users, n=5)
```

```{r, warning=FALSE}
books <- fread("BookDataset/books.csv", sep = ";")
head(books, n=5)
```


# Exploratory data analysis

### Exploration of unique number of books & users

```{r}
n_distinct(ratings$`User-ID`)
n_distinct(books$ISBN)
```

### Merging dataframes

```{r}
dataset = merge(ratings, users, by.x = "User-ID", by.y = "User-ID")
```

Retrieving country information

```{r}
dataset$Country <- sub('.*,\\s*','', dataset$Location)
head(dataset, n=5)
```

```{r}
dataset <- merge(dataset, books, by.x = "ISBN", by.y = "ISBN")
head(dataset, n=5)
```

Changing column names

```{r}
colnames(dataset)
```

```{r}
colnames(dataset)[which(colnames(dataset) %in%  c("User-ID", "Book-Rating", "Book-Title", "Book-Author", "Year-Of-Publication", "Image-URL-S", "Image-URL-M", "Image-URL-L"))] <- c("User.ID", "Book.Rating", "Book.Title", "Book.Author", "Year.Of.Publication", "Image.URL.S", "Image.URL.M", "Image.URL.L")
colnames(dataset)
```

Looking classes of data columns

```{r}
sapply(dataset, class)
```

Transforming age to numeric

```{r, warning=FALSE}
dataset <- transform(dataset, Age = as.numeric(Age))
head(dataset, n=5)
```

Check for duplications

```{r}
sum(duplicated(dataset))
```

Check for NA values

```{r}
sum(is.na(dataset))
sum(is.na(dataset$Age))
```

Turning ID columns to factor

```{r}
dataset$User.ID <- as.factor(dataset$User.ID)
dataset$ISBN <- as.factor(dataset$ISBN)
summary(dataset)
```

Number of ratings by users

```{r}
rating.count.users <- dataset %>% count(User.ID)
rating.count.users
```

```{r}
summary(rating.count.users)
```

Let's visualize number of ratings.

```{r}
dataset %>%
  group_by(Book.Rating) %>%
  summarize(cases = n()) %>%
  ggplot(aes(Book.Rating, cases)) + geom_col(color="gray") +
  theme_minimal() + scale_x_continuous(breaks = 0:10)
```

There are a lot of zero values. It might indicate the absence of rating. So we will remove those rows.

```{r}
dataset = dataset[dataset$Book.Rating!= 0, ]
```

```{r}
dataset %>%
  group_by(Book.Rating) %>%
  summarize(cases = n()) %>%
  ggplot(aes(Book.Rating, cases)) + geom_col(fill="orange") +
  theme_minimal() + scale_x_continuous(breaks = 0:10)
```


```{r}
rating.count.books <- dataset %>% count(ISBN)
rating.count.books
```


```{r}
summary(rating.count.books)
```
Check how many of them above the mean

```{r}
nrow(rating.count.books)
sum(rating.count.books$n > 2.562)
```

Order in descending order 

```{r}
rating.count.books[order(-n)]
```

Get the books which has higher rating more than the mean and classify by yes or no

```{r}
rating.count.books <- rating.count.books[rating.count.books$n > 2.562]
```


```{r}
dataset$Rating.Count.Above.Mean <- ifelse(dataset$ISBN %in% rating.count.books$ISBN, "Yes", "No")
```


```{r}
nrow(dataset[dataset$Rating.Count.Above.Mean == "Yes",])
nrow(dataset[dataset$Rating.Count.Above.Mean == "No",])
```


## Top 10 countries users from

```{r}
countries <- dataset %>% count(Country)
countries <- countries[!(countries$Country=="" | countries$Country=="n/a")]
countries <- countries[order(-n)][1:10]
countries
```

```{r}
countries %>% 
ggplot(aes(Country, n)) +
  geom_col(fill="brown")
```


### Top 10 highest rated books which has more than 100 votes


```{r}
ratings.book <- dataset %>% group_by(ISBN) %>% filter(n()>100)
ratings.mean <- setorder(setDT(ratings.book)[, .(Book.Rating = mean(Book.Rating)), by = Book.Title], -Book.Rating)[1:10]
ratings.mean
```


```{r}
ratings.mean %>% 
ggplot(aes(Book.Rating, Book.Title)) +
  geom_col(fill='pink')
```

## Exploration of year of publication

Minimum value of year is 0 which means we do not have the year information of that book. We will replace those 0 values with NA.

```{r}
summary(dataset$Year.Of.Publication)
```

```{r}
dataset$Year.Of.Publication[dataset$Year.Of.Publication == 0] <- NA
```


```{r}
summary(dataset$Year.Of.Publication)
```


```{r}
year_hist <- dataset %>%
    ggplot(aes(Year.Of.Publication)) +
    geom_histogram(binwidth=1, fill='purple') +
    theme(text = element_text(size = 20))

year_hist
```


## Exploration of Authors

Check how many unique author values we have

```{r}
length(dataset$Book.Author)
n_distinct(dataset$Book.Author)
```

## Top 10 rated authors

Get the authors which has been voted more than 100 times. Calculate rating means for each one and rate them in descending order. 

```{r}
author.high.count <-  dataset %>% group_by(Book.Author) %>% filter(n()>100)
author.high.count.mean <- setorder(setDT(author.high.count)[, .(Book.Rating = mean(Book.Rating)), by = Book.Author], -Book.Rating)[1:10]
author.high.count.mean
```


```{r}
author.high.count.mean %>% 
ggplot(aes(Book.Rating, Book.Author)) +
  geom_col(fill='blue')
```

## Check out for class imbalances


```{r}
table(dataset$Rating.Count.Above.Mean)
```

```{r}
prop.table(table(dataset$Rating.Count.Above.Mean))
```

Visualization of how many books has greater rating than the mean

```{r}
ggplot(dataset, aes(x=reorder(Rating.Count.Above.Mean, Rating.Count.Above.Mean, function(x)-length(x)))) +
geom_bar(fill='red') +  labs(x='Rating Count Above Mean')
```


```{r}
n_legit <- 239195
new_frac_legit <- 0.75
new_n_total <- n_legit/new_frac_legit
```

```{r}
oversampling_result <- ovun.sample(Rating.Count.Above.Mean ~ ., data = dataset, method = "over", 
                                   N = new_n_total, seed = 2018)
oversampled <- oversampling_result$data
table(oversampled$Rating.Count.Above.Mean)
```

```{r}
ggplot(oversampled, aes(x=reorder(Rating.Count.Above.Mean, Rating.Count.Above.Mean, function(x)-length(x)))) +
geom_bar(fill='red') +  labs(x='Oversampled Rating Count Above Mean')
```






----------------------------------------------------------------------

## Missing data imputation


```{r}
sum(is.na(dataset$Age))
```

We have 277845 NA values in Age column but MICE assumes missing at random values we will also introduce missing values with 'prodNA' artificially. Entries in the given dataframe are deleted completely at random up to the specified amount.

We will use Hmisc package for data imputation and the impute() function which simply imputes missing value using user defined statistical method (mean, max, mean). Its default is median. Let's introduce 1% of missing values to the dataset


```{r}
dataset.missing <- prodNA(dataset, noNA = 0.1)
summary(dataset.missing)
```

We will use mice function to impute missing values.
- m value refers to 5 imputed data sets
- maxit refers to no. of iterations taken to impute missing values
- method refers to method used in imputation. We used predictive mean matching which is for numeric variables because we the Age column has the most missing values.

```{r}
imputed.data <- mice(dataset.missing, m=5, maxit = 10, method = 'pmm', seed = 500)
summary(imputed.data)
```

As seen above there are 5 imputed datasets and we can select any of them using complete() function or we can combine the results from these models.

```{r}
imputed.data$imp$Age
```

```{r}
completeData <- complete(imputed.data,2)
```



We will build predictive model with with() function

```{r}
fit <- with(data = imputed.data, exp = lm(Book.Rating ~ Age + Year.Of.Publication)) 
fit
```

```{r}
combine <- pool(fit)
summary(combine)
```


----------------------------------------------------------------------












