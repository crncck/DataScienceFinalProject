---
title: "CENG 4515 DATA SCIENCE AND ANALYTICS"
author: "Ayşe Ceren Çiçek & Gizem Kurnaz"
date: "01/13/2021"
output: 
  pdf_document: default
  html_document: default
  word_document: default
subtitle: Final Project
font-family: Gill Sans
---

<script type="text/javascript">
  // When the document is fully rendered...
  $(document).ready(function() {
    // ...select all header elements...
    $('h1, h2, h3, h4, h5').each(function() {
      // ...and add an id to them corresponding to their 'titles'
      $(this).attr('id', $(this).html());
    });
  });
</script>

## TABLE OF CONTENTS

- <a href="#1) Dataset">1) Dataset</a><br>
- <a href="#2) Exploratory data analysis">2) Exploratory data analysis</a><br>
- <a href="#3) Visualization techniques">3) Visualization techniques</a><br>
- <a href="#Z.a) Imbalanced data set">Z.a) Imbalanced data set</a><br>
- <a href="#Y.a) Missing data imputation">Y.a) Missing data imputation</a><br>
- <a href="#4) Multicollinearity">4) Multicollinearity</a><br>
- <a href="#6) Logistic Regression">6) Logistic Regression</a><br>
- <a href="#5) PCA">5) PCA</a><br>
  - <a href="#6.X) Building a logistic regression model on the transformed data">6.X) PCA with Logistic Regression</a><br>
- <a href="#7) Clustering">7) Clustering</a><br>
  - <a href="#7.1) K-Means Clustering Algorithm Application">7.1) K-Means Clustering Algorithm Application</a><br>
  - <a href="#7.2) Hierarchical Clustering Algorithm Application">7.2) Hierarchical Clustering Algorithm Application</a><br>
    - <a href="#Y.b) Missing Data with Hierarchical Clustering
">Y.b) Missing Data with Hierarchical Clustering
</a><br>
- <a href="#8) Classification">8) Classification</a><br>
  - <a href="#8.1) Decision Tree Algorithm Application">8.1) Decision Tree Algorithm Application</a><br>
    - <a href="#Z.b) Decision tree with imbalanced data">Z.b) Decision tree with imbalanced data</a><br>
  - <a href="#8.2) K-Nearest Neighbors (K-NN) Algorithm Application">8.2) K-Nearest Neighbors Algorithm Application</a><br>


  


# 1) Dataset

**Dataset:** http://www2.informatik.uni-freiburg.de/~cziegler/BX/

The dataset contains three csv files which are ratings, users and books.

**Ratings.csv**: Contains the book rating information. Ratings (Book-Rating) are either explicit, expressed on a scale from 1-10 (higher values denoting higher appreciation), or implicit, expressed by 0. This file contains 999,999 rows and 3 columns:

- User.ID: User identification ID
- ISBN: ISBN Id for the book
- Book.Rating: Rating for each book given by user

**Users.csv**: Contains the users. User IDs (User-ID) have been anonymized and map to integers. Demographic data is provided (Location, Age) if available. Otherwise, these fields contain NULL-values. This file contains 278,858 rows and 3 columns: 

- User.ID: Unique identification ID for the user
- Location: Location of the user
- Age: Age of the user

**Books.csv**: Books are identified by their respective ISBN. Invalid ISBNs have already been removed from the dataset. Moreover, some content-based information is given (Book-Title, Book-Author, Year-Of-Publication, Publisher), obtained from Amazon Web Services. In case of several authors, only the first is provided. URLs linking to cover images are also given, appearing in three different flavours (Image-URL-S, Image-URL-M, Image-URL-L), i.e., small, medium, large. These URLs point to the Amazon web site. This file contains 271,379 rows and 8 columns:

- ISBN: ISBN ID for each book
- Book.Title: Title of the book
- Book.Author: Author name
- Year.Of.Publication: Publication year
- Publisher: Name of the publisher
- Image.URL.S: Small sized image URL of the book
- Image.URL.M: Medium sized image URL of the book
- Image.URL.L: Large sized image URL of the book

## Importing libraries

```{r, warning=FALSE,message=FALSE,error=FALSE, results='hide'}
library(funModeling) 
library(tidyverse) 
library(dplyr)
library(data.table)
library(stringr)
library(ggplot2)
library(Hmisc)
library(missForest)
library(mice)
library(ROSE)
library(cluster)
library(ClusterR)
library(caTools)
library(caret)
library(knitr)
library(Amelia)
library(rpart)
library(ggcorrplot)
library(factoextra)
library(class)
```


## Loading dataset

We will load 3 data files first. 

```{r, warning=FALSE}
ratings <- fread("BookDataset/ratings.csv", sep = ";")
head(ratings, n=5)
```

```{r,warning=FALSE}
users <- fread("BookDataset/users.csv", sep = ";")
head(users, n=5)
```

```{r, warning=FALSE, message=FALSE, error=FALSE}
books <- fread("BookDataset/books.csv", sep = ";")
head(books, n=5)
```


# 2) Exploratory data analysis

Let's check the unique number of books & users. 

```{r}
n_distinct(ratings$`User-ID`)
n_distinct(books$ISBN)
```


### Merging dataframes

We will first merge ratings and users dataframes based on User-ID column.

```{r}
dataset = merge(ratings, users, by.x = "User-ID", by.y = "User-ID")
```

Now we will retrieve country information from the Location column which contains state, city, country information.

```{r}
dataset$Country <- sub('.*,\\s*','', dataset$Location)
dataset <- dataset[(which(nchar(dataset$Country) >= 2)),]
head(dataset, n=5)
```

We will merge the last dataframe with our new one based on ISBN.

```{r}
dataset <- merge(dataset, books, by.x = "ISBN", by.y = "ISBN")
head(dataset, n=5)
```

### Changing column names

We have 13 columns and we will change name of the some of them.

```{r}
colnames(dataset)
```

```{r}
colnames(dataset)[which(colnames(dataset) %in%  c("User-ID", "Book-Rating", "Book-Title", "Book-Author", "Year-Of-Publication", "Image-URL-S", "Image-URL-M", "Image-URL-L"))] <- c("User.ID", "Book.Rating", "Book.Title", "Book.Author", "Year.Of.Publication", "Image.URL.S", "Image.URL.M", "Image.URL.L")
colnames(dataset)
```

Now we can check the classes of those columns.

```{r}
sapply(dataset, class)
```

Except for the Age column, it seems like others' have appropriate classes. We will turn Age column to numeric.

```{r, warning=FALSE}
dataset <- transform(dataset, Age = as.numeric(Age))
head(dataset, n=5)
```

Check for any duplications

```{r}
sum(duplicated(dataset))
```

Also, we will turn ID columns into factors to make easier usage in the future.

```{r}
dataset$User.ID <- as.factor(dataset$User.ID)
dataset$ISBN <- as.factor(dataset$ISBN)
summary(dataset)
```

## Number of ratings by users

```{r}
rating.count.users <- dataset %>% count(User.ID)
head(rating.count.users, n=5)
```

From the table below, we can see that the maximum number of rating by a user is 2899 and the mean is about 5.196.

```{r}
summary(rating.count.users)
```

Let's visualize number of ratings.

```{r}
dataset %>%
  group_by(Book.Rating) %>%
  summarize(cases = n()) %>%
  ggplot(aes(Book.Rating, cases)) + geom_col(color="gray") +
  theme_minimal() + scale_x_continuous(breaks = 0:10)
```

There are a lot of zero values. It might indicate the absence of rating. So we will remove those rows.

```{r}
dataset <- dataset[dataset$Book.Rating!= 0, ]
```

# 3) Visualization techniques

So the rating change as seen below. 8 is the most used voting value.

```{r}
dataset %>%
  group_by(Book.Rating) %>%
  summarize(cases = n()) %>%
  ggplot(aes(Book.Rating, cases)) + geom_col(fill="orange") +
  theme_minimal() + scale_x_continuous(breaks = 0:10)
```

We do not have a categorical variable but we will create one based on average votes for each book and classify them as yes if its average rating is greater than the overall mean.


Now we will get average rating for each book.

```{r}
books.rating.mean <- aggregate(Book.Rating ~ ISBN, dataset, mean)
head(books.rating.mean, n=5)
```

The highest rate a book has is 10. The mean value is 7.529.

```{r}
summary(books.rating.mean)
```

Now we will get the books which has more average rating than the mean and classify them as above the mean or not.

```{r}
nrow(books.rating.mean)
sum(books.rating.mean$Book.Rating > 7.529)
```

Let order it in descending order.

```{r}
book.rating.mean <-  books.rating.mean[order(-books.rating.mean$Book.Rating),]
```


Get the books which has higher rating more than the mean and classify by yes or no.

```{r}
books.rating.mean <- books.rating.mean[books.rating.mean$Book.Rating > 7.692,]
```


```{r}
dataset$Rating.Count.Above.Mean <- ifelse(dataset$ISBN %in% books.rating.mean$ISBN, "Yes", "No")
```

36151 books has count more than mean and 32748 are below the mean.

```{r}
nrow(dataset[dataset$Rating.Count.Above.Mean == "Yes",])
nrow(dataset[dataset$Rating.Count.Above.Mean == "No",])
```


## Top 10 countries 

Top 10 countries that are users from.

```{r}
countries <- dataset %>% count(Country)
countries <- countries[!(countries$Country=="n/a")]
countries <- countries[order(-n)][1:10]
head(countries, n=10)
```

```{r}
countries %>% 
ggplot(aes(Country, n)) +
  geom_col(fill="brown")
```

The users are mostly from the USA. The second country is Germany and Canada is the third one.

## Top 10 highest rated books which has more than 100 votes


```{r}
ratings.book <- dataset %>% group_by(ISBN) %>% filter(n()>100)
ratings.mean <- setorder(setDT(ratings.book)[, .(Book.Rating = mean(Book.Rating)), by = Book.Title], -Book.Rating)[1:10]
ratings.mean
```


```{r}
ratings.mean %>% 
ggplot(aes(Book.Rating, Book.Title)) +
  geom_col(fill='pink')
```

Harry Potter and the Order of the Phoenix (Book 5) is the one that has the highest rating in books which has more votes than 100. And Harry Potter and the Sorcerer's Stone (Harry Potter (Paperback)) is the second one with an 8.92 rates.


## Exploration of year of publication

Minimum value of year is 0 which means we do not have the year information of that book. We will replace those 0 values with NA.

```{r}
summary(dataset$Year.Of.Publication)
```

```{r}
dataset$Year.Of.Publication[dataset$Year.Of.Publication == 0] <- NA
```


```{r}
summary(dataset$Year.Of.Publication)
```
The graph shows the year of publication data of books. 

```{r}
year_hist <- dataset %>%
    ggplot(aes(Year.Of.Publication)) +
    geom_histogram(binwidth=1, fill='purple') +
    theme(text = element_text(size = 20))

year_hist
```


## Exploration of Authors

Check how many unique author values we have

```{r}
length(dataset$Book.Author)
n_distinct(dataset$Book.Author)
```
There are 21,926 unique authors.

### Top 10 rated authors

Get the authors which has been voted more than 100 times. Calculate rating means for each one and rate them in descending order. 

```{r}
author.high.count <-  dataset %>% group_by(Book.Author) %>% filter(n()>100)
author.high.count.mean <- setorder(setDT(author.high.count)[, .(Book.Rating = mean(Book.Rating)), by = Book.Author], -Book.Rating)[1:10]
author.high.count.mean
```


```{r}
author.high.count.mean %>% 
ggplot(aes(Book.Rating, Book.Author)) +
  geom_col(fill='blue')
```

J. K. Rowling has the highest rate with 8.93. Neil Gaiman and J. R. R. Tolkien are the second and the third ones.

# Z.a) Imbalanced data set

We will look at how much data we have by class.

```{r}
table(dataset$Rating.Count.Above.Mean)
```

It seems that the Yes class has more data.

```{r}
prop.table(table(dataset$Rating.Count.Above.Mean))
```

Visualization of how many books has greater rating than the mean

```{r}
ggplot(dataset, aes(x=reorder(Rating.Count.Above.Mean, Rating.Count.Above.Mean, function(x)-length(x)))) +
geom_bar(fill='red') +  labs(x='Rating Count Above Mean')
```

We will store the old dataset as imbalanced dataset.

```{r}
imbalanced.dataset <- dataset
missing.dataset <- dataset
```

Now we can apply oversampling to make them equal size.

```{r}
n_legit <- 36151
new_frac_legit <- 0.68
new_n_total <- n_legit/new_frac_legit
```

```{r}
oversampling_result <- ovun.sample(Rating.Count.Above.Mean ~ ., data = dataset, method = "over", 
                                   N = new_n_total, seed = 2018)
dataset <- oversampling_result$data
row.names(dataset) <- NULL
table(dataset$Rating.Count.Above.Mean)
```

Now our classes have almost the same number of data.

```{r}
prop.table(table(dataset$Rating.Count.Above.Mean))
```


```{r}
ggplot(dataset, aes(x=reorder(Rating.Count.Above.Mean, Rating.Count.Above.Mean, function(x)-length(x)))) +
geom_bar(fill='red') +  labs(x='Oversampled Rating Count Above Mean')
```

# Y.a) Missing data imputation 

Check for NA values

```{r}
sum(is.na(dataset))
sum(is.na(imbalanced.dataset))
names(which(colSums(is.na(dataset)) > 0))
names(which(colSums(is.na(imbalanced.dataset)) > 0))
```

We have missing values and which are in the Age and Year of Publication columns. We will use mean imputation (or mean substitution) that replaces missing values of a certain variable by the mean of non-missing cases of that variable.


```{r}
dataset$Age <- impute(dataset$Age, mean)
dataset$Year.Of.Publication <- impute(dataset$Year.Of.Publication, mean)

imbalanced.dataset$Age <- impute(imbalanced.dataset$Age, mean)
imbalanced.dataset$Year.Of.Publication <- impute(imbalanced.dataset$Year.Of.Publication, mean)
```


Now, let's check again for NA values 


```{r}
sum(is.na(dataset))
sum(is.na(imbalanced.dataset))
```

We have fixed missing data.

## Subset data

The dataset contains 53,163 values after data preprocessing. Because we had some performance issues with our machines while fitting models, we had to take a subset of the dataset with 500 rows.

```{r}
set.seed(12345)
dataset <- dataset[sample(1:nrow(dataset),500),]
imbalanced.dataset <- imbalanced.dataset[sample(1:nrow(dataset),500),]
missing.dataset <- missing.dataset[sample(1:nrow(dataset),500),]
row.names(dataset) <- NULL
row.names(imbalanced.dataset) <- NULL
row.names(missing.dataset) <- NULL
nrow(dataset)
```

# 4) Multicollinearity

We replaced Rating.Count.Above.Mean values Yes with 2 and No with 1.

```{r}
dataset$Rating.Count.Above.Mean <- ifelse(dataset$Rating.Count.Above.Mean == "No", 1, 2)
```

We selected certain columns of the data to calculate the correlation.

```{r}
data <-dataset[, c('Book.Rating',"Age","Year.Of.Publication","Rating.Count.Above.Mean")]
```


```{r}
sapply(data, class)
```

We deleted NA values.

```{r}
data <- data[!is.na(data$Age),]
data <- data[!is.na(data$Year.Of.Publication),]
data <- data[!is.na(data$Book.Rating),]
data <- data[!is.na(data$Rating.Count.Above.Mean),]
```

We drew a correlation map to see the correlation between our columns.

```{r}
correlations <- cor(data)
corrplot::corrplot(correlations,method = "square",tl.cex = 0.6, tl.col = "black")
```

Multicollinearity occurs when features are highly correlated with one or more of the other features in the dataset.

As you see above some of the features in the dataset are highly correlated with each other. So, there exists multicollinearity. We can effectively eliminate multicollinearity between features using PCA.

### We will apply PCA (5) after Logistic Regression and we will also apply PCA to Logistic Regression.

# 6) Logistic Regression

Logistic Regression is a classification model which is used to understand the relationship between the dependent variable and one or more independent variables by estimating probabilities using a logistic regression equation. 

- The dependent variable should be binary like yes or no. 

- It can help you predict the likelihood of an event happening or a choice being made.

Linear Regression outputs continuous value, and it has a straight line to map the input variables to the dependent variables. The output can be any of an infinite number of possibilities. On the other hand, Logistic Regression uses a logistic function to map the input variables to categorical dependent variables. In contrast to Linear Regression, Logistic Regression outputs a probability between 0 and 1.

## 6.a) Functions and arguments

We have a factor variable which is Rating.Count.Above.Mean (our dependent variable) but R assumes it is numeric. We modified it to factor.

```{r}
data$Rating.Count.Above.Mean <- as.factor(data$Rating.Count.Above.Mean)
```

We split our data into train and test data.

```{r}
sample<- createDataPartition(y= data$Rating.Count.Above.Mean,p=0.8,list = FALSE)

train_data <- data[sample,]
test_data <- data[-sample,]
```

We created our model.

```{r}
logistic_model <- glm(Rating.Count.Above.Mean~.,data = train_data,family = "binomial")
```

We made predictions.

```{r}
prob <- predict(logistic_model,newdata=test_data,type="response")
pred <- ifelse(prob > 0.5, 2, 1)
```

We generated a confusion matrix.

```{r}
conf.matrix <- confusionMatrix(test_data$Rating.Count.Above.Mean,as.factor(pred))
conf.matrix
```

## 6.b) Visualization

```{r}
# Heatmap visualization of confusion matrix
table <- data.frame(conf.matrix$table)
plotTable <- table %>%
  group_by(Prediction) %>%
  mutate(prop = Freq/sum(Freq))
ggplot(data =  plotTable, mapping = aes(x = Reference, y = Prediction, alpha = prop)) +
  geom_tile(aes(fill = Freq), colour = "white") +
  geom_text(aes(label = Freq), vjust = .5, fontface  = "bold", alpha = 1, color="white", size=10) +
  scale_fill_gradient(low = "blue", high = "navyblue") +
  theme_bw() + theme(legend.position = "none")
```

## 6.c) 
As you see above model accuracy is 83%. The model classified 39 + 44 datapoint correctly. We can also increase this ratio by applying PCA(Principal Component Analysis) to our dataset.

# 5) PCA

## 5.a) Functions and arguments

We converted our columns to numeric to apply PCA.

```{r}
train_data$Book.Rating <- as.numeric(train_data$Book.Rating)
train_data$Year.Of.Publication <- as.numeric(train_data$Year.Of.Publication)
train_data$Rating.Count.Above.Mean <- as.numeric(train_data$Rating.Count.Above.Mean)
test_data$Book.Rating <- as.numeric(test_data$Book.Rating)
test_data$Year.Of.Publication <- as.numeric(test_data$Year.Of.Publication)
test_data$Rating.Count.Above.Mean <- as.numeric(test_data$Rating.Count.Above.Mean)
```


```{r}
sapply(train_data,class)
levels(train_data$Rating.Count.Above.Mean)
```

### Apply PCA

We applied Pca in our dataset.

```{r}
pca <- prcomp(train_data, center = TRUE, scale = TRUE)
pca_test<-prcomp(test_data,center = TRUE, scale=TRUE)
pca
```

## 5.b) Visualization

```{r}
plot(pca, type='l', main="PCA - Principal Components Analysis Chart", col="red")
```


```{r}
cumpro <- cumsum(pca$sdev^2 / sum(pca$sdev^2))
plot(cumpro[0:15], xlab = "PC #", ylab = "Amount of explained variance", main = "Cumulative variance plot")
abline(v = 6, col="red", lty=5)
abline(h = 0.88759, col="red", lty=5)
legend("topleft", legend=c("Cut-off @ PC6"),
       col=c("red"), lty=5, cex=0.6)
```

### Get eigenvalues

```{r}
explained.variance <- pca$sdev^2
explained.variance
```

### Plot PCA

```{r}
pca.var <- get_pca_var(pca)

kmean <- kmeans(pca.var$coord, centers = 3, nstart=25)
group <- as.factor(kmean$cluster)

fviz_pca_var(pca, col.var=group, palette='jco', legend.title='Cluster')
```

After applying it to PCA, we drew a correlation map again.

```{r}
correlations <- cor(pca$x[,c(0:4)])

corrplot::corrplot(correlations,method = "square", tl.col = "black")
```

## 5.c) Final comments

We cannot see any correlation between components. This is because of PCA has transformed the set of correlated variables in the original dataset into a set of uncorrelated variables.


# 6.X) Building a logistic regression model on the transformed data

After applying PCA, we applied logistic regression to our data again to compare the results and see the effect of PCA.

```{r}
train_data$Rating.Count.Above.Mean <- as.factor(train_data$Rating.Count.Above.Mean)
```

```{r}
set.seed(42)

data_pca <- data.frame(Rating.Count.Above.Mean=train_data[,"Rating.Count.Above.Mean"],pca$x[,0:4])
head(data_pca)
```

```{r}
set.seed(42)
model_pca <- glm(Rating.Count.Above.Mean ~ .,data= data_pca,family = binomial)

test_data_pca <- predict(pca,newdata = test_data)
```


```{r}
prob <- predict(model_pca , newdata = data.frame(test_data_pca[,0:4]),type = "response")

pred <- factor(ifelse(prob>0.5,2,1))

levels(as.factor(pred))
levels(test_data$Rating.Count.Above.Mean)

confusionMatrix(as.factor(test_data$Rating.Count.Above.Mean),as.factor(pred))
```

As you see above our accuracy become 100%.The test accuracy has increased by 17%. Both false positives and false negatives have also been reduced. The reason behind the performance increase in this model is is PCA has effectively eliminated the multicollinearity.


# 7) Clustering

## 7.a) Why K-means and Hierarchical Clustering

We will cluster users based on their age and book ratings.

We decided to use K-means Clustering and Hierarchical Clustering algorithms. The main reason is that we learned them in the lesson and they are the most popular ones. Besides those reasons there are some other reasons:

**Advantages of K-Means:**

- Guarantees convergence
- Can warm-start the positions of centroids
- Easily adapts to new examples
- Generalizes to clusters of different shapes and sizes, such as elliptical clusters

**Advantages of Hierarchical Clustering:**

- It is a powerful technique that allows building tree structures from data similarities
- It lets us see how different sub-clusters relate to each other, and how far apart data points are

# 7.1) K-Means Clustering Algorithm Application

K-means clustering aims to partition data into k clusters in a way that data points in the same cluster are similar and data points in the different clusters are farther apart. It’s an unsupervised machine learning algorithm. It computes the centroids and iterates until it finds optimal centroid. It assumes that the number of clusters are already known. 

The k-means clustering works as follows:

- Choose the k number of clusters

- Select k random points, the centroids (they don’t have to be part of the dataset)

- Assign each point to the closest centroid

- Compute and replace the new centroid of each cluster

- Reassign each data point to the new closest centroid. If any reassignment happens, go back to previous step. 

### 7.1.a) Functions and arguments

We will use age and book rating to cluster users.

```{r}
X <- dataset[sample,] %>% select("User.ID", "Age", "Book.Rating")
head(X, n=5)
```

#### Determining Optimal Number of Clusters

The number of clusters that we choose for a given dataset cannot be random. Each cluster is formed by calculating and comparing the distances of data points within a cluster to its centroid. An ideal way to figure out the right number of clusters would be to calculate the Within-Cluster-Sum-of-Squares (WCSS). 

**WCSS** is the sum of squares of the distances of each data point in all clusters to their respective centroids.

We can use some techniques to determine optimal number of clusters. **Elbow method** is one of them. The method consists of plotting the explained variation as a function of the number of clusters, and picking the elbow of the curve as the number of clusters to use.


```{r}
X <- X[!is.na(X$Age), ]
```


#### Elbow method

We are going to use the Elbow Method to decide the optimal number of clusters.

```{r}
set.seed(6)
wcss <- vector() 
for (i in 1:10) wcss[i] <-  sum(kmeans(X, i)$withinss)
plot(1:10, wcss, type = "b", main = paste("Clusters of users"), xlab = "Number of clusters", ylab = "WCSS")
```


As seen on the plot, the optimal number of clusters seems as 3.

#### Apply K-Means

We will split our data into 3 clusters. The nstart parameter attempts multiple initial configurations and reports on the best one. 

```{r}
set.seed(29)
kmeans.model <- kmeans(X, 3, iter.max = 300, nstart = 10)
kmeans.model
```

### 7.1.b) Visualization

With clusplot function we can draw a 2 dimensional clustering plot with our clusters.

```{r}
clusplot(X, clus = kmeans.model$cluster, lines = 0, shade = TRUE, color = TRUE, labels = 2, plotchar = FALSE, span = TRUE, main = paste("Clusters of users"), xlab = "Age", ylab = "Book Rating")
```

### 7.1.c) 
We can see the clusters above. Age and book ratings explain 68.34% of the point variability. There are some far points like 139 and 1 which have very low age and very high book ratings. We assumed that they can be outliers. Users might not enter the correct age and might give very few votes with a high rating value.


# 7.2) Hierarchical Clustering Algorithm Application

Hierarchical clustering is **an algorithm that groups similar objects into groups called clusters**. It is an alternative approach to k-means clustering for identifying groups. The endpoint is a set of clusters, where each cluster is distinct from the other cluster, and the objects within each cluster are broadly similar to each other. 

- It is an unsupervised machine learning algorithm. 
- The hierarchical clustering does not require us to pre-specify the number of clusters to be generated as is required by the k-means approach. 
- It has a tree-based representation called **dendrogram** which is a diagram that shows the hierarchical relationship between objects.


Hierarchical clustering can be divided into two main types: **agglomerative** and **divisive**. 

**A) Agglomerative Clustering**

It is a bottom-up approach. In the beginning, each object is initially considered as a single-element cluster. At each step of the algorithm, the two clusters that are the most similar are combined into a new bigger cluster. This procedure is iterated until all points are member of just one single big cluster. 

This process has a **O(N^3) time complexity** and a **O(N^2) memory complexity** that makes it *not tractable for large datasets*.

How it works:

- Make each data point a single-point cluster
- Take the two closest *data points* and make them one cluster
- Take the two closest *clusters* and make them one cluster
- Repeat the previous step until there is only one cluster


**B) Divisive Clustering**

It is a top-down approach. It begins with the root, in which all objects are included in a single cluster. At each step of iteration, the most heterogeneous cluster is divided into two. The process is iterated until all objects are in their own cluster.

This process requires at each iteration to search for the best split, implying a **O(2N) time complexity** that has to be tackled with some heuristics. *Divisive hierarchical clustering is good at identifying large clusters.*

### 7.2.a) Functions and arguments

We will use the same data frame that we have been created in the previous model.

```{r}
head(X, n=5)
```

#### Dendrogram

A dendrogram is a tree-like chart that shows the sequences of merges or splits of clusters. We will use it to find the optimal number of clusters.

```{r}
dendrogram <- hclust(dist(X, method = 'euclidean'), method = 'ward.D')
plot(dendrogram, main = 'Dendrogram', xlab = 'Users', ylab = 'Euclidean distances')
```

The larger gap cut generates 2 clusters so we can say optimal number of clusters is 2.


#### Apply hierarchical clustering

```{r}
hc <- hclust(dist(X, method = 'euclidean'), method = 'ward.D')
```

Cutree method cuts a dendrogram tree into several groups by specifying the desired number of clusters k(s), or cut height(s).

```{r}
y_hc <- cutree(hc, 2)
y_hc
```

We can see the clusters above.

### 7.2.b) Visualize the clusters

With clusplot function we can draw a 2 dimensional clustering plot with our clusters.

```{r}
clusplot(X, clus = y_hc, lines = 0, shade = TRUE, color = TRUE, labels = 2, plotchar = FALSE, span = TRUE, 
         main = paste("Clusters of clients"), xlab = "Age", ylab = "Book Rating")
```

### 7.2.c) 
With hierarchical clustering, we have almost the same clusters. But we divided the data into 2 clusters here. The blue cluster has the same data as the blue and pink clusters of k-means. There are also extreme points which are 139 and 1.

# Y.b) Missing Data with Hierarchical Clustering

```{r}
X <- missing.dataset[sample,] %>% select("User.ID", "Age", "Book.Rating")
head(X, n=5)
```
#### Dendrogram

A dendrogram is a tree-like chart that shows the sequences of merges or splits of clusters. We will use it to find the optimal number of clusters.

```{r}
dendrogram <- hclust(dist(X, method = 'euclidean'), method = 'ward.D')
plot(dendrogram, main = 'Dendrogram', xlab = 'Users', ylab = 'Euclidean distances')
```

The larger gap cut generates 2 clusters so we can say optimal number of clusters is 2.


#### Apply hierarchical clustering

```{r}
hc <- hclust(dist(X, method = 'euclidean'), method = 'ward.D')
```

Cutree method cuts a dendrogram tree into several groups by specifying the desired number of clusters k(s), or cut height(s).

```{r}
y_hc <- cutree(hc, 2)
y_hc
```

We can see the clusters above.

### Visualize the clusters

With clusplot function we can draw a 2 dimensional clustering plot with our clusters.

```{r}
clusplot(X, clus = y_hc, lines = 0, shade = TRUE, color = TRUE, labels = 2, plotchar = FALSE, span = TRUE, 
         main = paste("Clusters of clients"), xlab = "Age", ylab = "Book Rating")
```

We tried to cluster data with missing values but algorithm displaced them by the median of the corresponding variable(s).

## 7.b) 
Those two clustering algorithms have almost the same results. Except that we divided the data into 2 clusters in hierarchical, which was 3 on kmeans. They both explain the variability on the same percentage.

# 8) Classification

## 8.a) Why Decision Tree and KNN

We will classify books as their average rating will be grater than the mean of overall ratings or not. We will classify them based on Book.Rating and Year.Of.Publication.

The reason we use Decision Tree is that the decision trees' outputs are easy to read and interpret without requiring and less data cleaning is required. Also, we needed high accuracy in the classification we will apply, so we decided that KNN is one of the best choices.

# 8.1) Decision Tree Algorithm Application

Decision Tree is a supervised learning technique that can be used for both classification and regression problems, but mostly it is preferred for solving classification problems. 
- It is a tree-structured classifier
- Internal nodes represent the features of a dataset
- Branches represent the decision rules
- Each leaf node represents the outcome

### 8.1.a) Functions and arguments

```{r}
Z <- dataset[sample,] %>% select("Book.Rating", "Year.Of.Publication", "Rating.Count.Above.Mean")
head(Z, n=5)
```

Turn the target feature to factor

```{r}
Z$Rating.Count.Factor <- factor(Z$Rating.Count.Above.Mean, levels = c(1, 2))
```


```{r}
Z <- Z[!is.na(Z$Year.Of.Publication), ]
Z <- select(Z,-c(Rating.Count.Above.Mean))
head(Z, n=5)
```


#### Split data into Train and Test set

Rating.Count.Factor column is our dependent variable.

```{r}
set.seed(123)
splitted <- sample.split(Z$Rating.Count.Factor, SplitRatio = 0.75)
train_Set <- subset(Z, splitted == TRUE)
test_Set <- subset(Z, splitted == FALSE)
```

#### Feature Scaling

Feature scaling is a method used to normalize the range of independent variables or features of data.

We will scale all the features except our dependent variable, Rating.Count.Factor.


```{r}
train_y = train_Set[,3]
test_y = test_Set[,3]

row.names(train_Set) <- NULL
row.names(test_Set) <- NULL

# Scaled test and train set
trainSet = data.frame(scale(train_Set[,-3]))
trainSet$Rating.Count.Factor = train_y

testSet = data.frame(scale(test_Set[,-3]))
testSet$Rating.Count.Factor = test_y
```

#### Apply Decision Tree

```{r}
model.decision <- rpart(formula = Rating.Count.Factor ~ ., data = trainSet)
model.decision
```


#### Prediction

Probability prediction show us predicted probabilities that the book will be classified as above the mean or not.

```{r}
probability.prediction <- predict(model.decision, newdata = testSet[-3,], type = 'class')
probability.prediction
```

#### Confusion Matrix

```{r}
levels(as.factor(probability.prediction))
levels(test_Set$Rating.Count.Factor)
conf.matrix <- confusionMatrix(as.factor(testSet[2:100, 3]),as.factor(probability.prediction))
decision.accuracy.balanced <- conf.matrix$overall['Accuracy']
conf.matrix
```

The accuracy is 85%. We have 5 + 9 incorrect classifications.


```{r}
# Heatmap visualization of confusion matrix
table <- data.frame(conf.matrix$table)
plotTable <- table %>%
  group_by(Prediction) %>%
  mutate(prop = Freq/sum(Freq))
ggplot(data =  plotTable, mapping = aes(x = Reference, y = Prediction, alpha = prop)) +
  geom_tile(aes(fill = Freq), colour = "white") +
  geom_text(aes(label = Freq), vjust = .5, fontface  = "bold", alpha = 1, color="white", size=10) +
  scale_fill_gradient(low = "blue", high = "navyblue") +
  theme_bw() + theme(legend.position = "none")
```

## 8.1.b) Visualization

### Visualize Train Set Results

```{r}
set <- trainSet
X1 <- seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 <- seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set <- expand.grid(X1, X2)
colnames(grid_set) <- c('Book.Rating', 'Year.Of.Publication')
y_grid <- predict(model.decision, newdata = grid_set, type = 'class')
plot(set[, -3], main = 'Decision Tree (Train Set)',
     xlab = 'Book.Rating', ylab = 'Year.Of.Publication',
     xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'deepskyblue', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'blue3', 'red3'))
```


### Visualize Test Set Results

```{r}
set <- testSet
X1 <- seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 <- seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set <- expand.grid(X1, X2)
colnames(grid_set) <- c('Book.Rating', 'Year.Of.Publication')
y_grid <- predict(model.decision, newdata = grid_set, type = 'class')
plot(set[, -3], main = 'Decision Tree (Test Set)',
     xlab = 'Book.Rating', ylab = 'Year.Of.Publication',
     xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'deepskyblue', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'blue3', 'red3'))
```


## 8.1.c) 
As we see above the books that are classified as above the mean are mostly on the red side of the plot. Most of the books have a higher year of publication. The model classified true most of the data.



# Z.b) Decision tree with imbalanced data

```{r}
imbalanced.dataset$Rating.Count.Above.Mean <- ifelse(imbalanced.dataset$Rating.Count.Above.Mean == "No", 1, 2)
```


```{r}
K <- imbalanced.dataset[sample,] %>% select("Book.Rating", "Year.Of.Publication", "Rating.Count.Above.Mean")
row.names(K) <- NULL
head(K, n=5)
```

Turn the target feature to factor

```{r}
K$Rating.Count.Factor <- factor(K$Rating.Count.Above.Mean, levels = c(1, 2))
```


```{r}
K <- K[!is.na(K$Year.Of.Publication), ]
K <- select(K,-c(Rating.Count.Above.Mean))
head(K, n=5)
```


### Split data into Train and Test set

Rating.Count.Factor column is our dependent variable.

```{r}
set.seed(123)
splitted <- sample.split(K$Rating.Count.Factor, SplitRatio = 0.75)
train_Set <- subset(K, splitted == TRUE)
test_Set <- subset(K, splitted == FALSE)
```

### Feature Scaling

Feature scaling is a method used to normalize the range of independent variables or features of data.

We will scale all the features except our dependent variable, Rating.Count.Factor.


```{r}
train_y = train_Set[,3]
test_y = test_Set[,3]

row.names(train_Set) <- NULL
row.names(test_Set) <- NULL

# Scaled test and train set
trainSet = data.frame(scale(train_Set[,-3]))
trainSet[,3] = train_y

testSet = data.frame(scale(test_Set[,-3]))
testSet[,3] = test_y
```



### Apply Decision Tree

```{r}
model.decision <- rpart(formula = Rating.Count.Factor ~ ., data = trainSet)
model.decision
```


### Prediction

Probability prediction show us predicted classes of a book if its average rating is above the mean or not.

```{r}
probability.prediction <- predict(model.decision, newdata = testSet[-3,], type = 'class')
probability.prediction
```

### Confusion Matrix

```{r}
levels(as.factor(probability.prediction))
levels(test_Set$Rating.Count.Factor)
conf.matrix <- confusionMatrix(as.factor(testSet[2:100, 3]),as.factor(probability.prediction))
decision.accuracy.imbalanced <- conf.matrix$overall['Accuracy']
conf.matrix
```

The accurary is 52%. We have 16 + 31 incorrect classifications.


```{r}
# Heatmap visualization of confusion matrix
table <- data.frame(conf.matrix$table)
plotTable <- table %>%
  group_by(Prediction) %>%
  mutate(prop = Freq/sum(Freq))
ggplot(data =  plotTable, mapping = aes(x = Reference, y = Prediction, alpha = prop)) +
  geom_tile(aes(fill = Freq), colour = "white") +
  geom_text(aes(label = Freq), vjust = .5, fontface  = "bold", alpha = 1, color="white", size=10) +
  scale_fill_gradient(low = "blue", high = "navyblue") +
  theme_bw() + theme(legend.position = "none")
```

### Visualize Train Set Results

```{r}
set <- trainSet
X1 <- seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 <- seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set <- expand.grid(X1, X2)
colnames(grid_set) <- c('Book.Rating', 'Year.Of.Publication')
y_grid <- predict(model.decision, newdata = grid_set, type = 'class')
plot(set[, -3], main = 'Decision Tree with Imbalanced Data (Train Set)',
     xlab = 'Book.Rating', ylab = 'Year.Of.Publication',
     xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'deepskyblue', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'blue3', 'red3'))
```


### Visualize Test Set Results

```{r}
set <- testSet
X1 <- seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 <- seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set <- expand.grid(X1, X2)
colnames(grid_set) <- c('Book.Rating', 'Year.Of.Publication')
y_grid <- predict(model.decision, newdata = grid_set, type = 'class')
plot(set[, -3], main = 'Decision Tree with Imbalanced Data(Test Set)',
     xlab = 'Book.Rating', ylab = 'Year.Of.Publication',
     xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'deepskyblue', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'blue3', 'red3'))
```

### Compare balanced data accuracy vs. imbalanced data accuracy

```{r}
print("Accuracy of decision tree with balanced data")
decision.accuracy.balanced

print("Accuracy of decision tree with imbalanced data")
decision.accuracy.imbalanced
```

As seen above balanced data has higher accuracy than the imbalanced data. 


# 8.2) K-Nearest Neighbors (K-NN) Algorithm Application

### 8.2.a) Functions and Arguments

```{r}
K <- dataset[sample,] %>% select("Book.Rating", "Year.Of.Publication", "Rating.Count.Above.Mean")
head(K, n=5)
```

Turn the target feature to factor

```{r}
K$Rating.Count.Factor <- factor(K$Rating.Count.Above.Mean, levels = c(1, 2))
```


```{r}
K <- K[!is.na(K$Year.Of.Publication), ]
K <- select(K,-c(Rating.Count.Above.Mean))
head(K, n=5)
```


#### Split data into Train and Test set

Rating.Count.Factor column is our dependent variable.

```{r}
set.seed(123)
splitted <- sample.split(K$Rating.Count.Factor, SplitRatio = 0.75)
train_Set <- subset(K, splitted == TRUE)
test_Set <- subset(K, splitted == FALSE)
```

### Feature Scaling

Feature scaling is a method used to normalize the range of independent variables or features of data.

We will scale all the features except our dependent variable, Rating.Count.Factor.


```{r}
train_y <- train_Set[,3]
test_y <- test_Set[,3]

# Scaled test and train set
trainSet <- data.frame(scale(train_Set[,-3]))
trainSet[,3] <- train_y

testSet <- data.frame(scale(test_Set[,-3]))
testSet[,3] <- test_y
```


#### Apply KNN


```{r}
y_pred <- knn(train = trainSet[, -3], test = testSet[, -3], cl = trainSet[, 3], k = 5, prob = TRUE)
y_pred
```

#### Confusion Matrix

```{r}
conf.matrix <- confusionMatrix(as.factor(testSet[, 3]),as.factor(y_pred))
knn.accuracy<- conf.matrix$overall['Accuracy']
conf.matrix
```

The accurary is 83%. We have 11 + 6 incorrect classifications.

### 8.2.b) Visualization

```{r}
# Heatmap visualization of confusion matrix
table <- data.frame(conf.matrix$table)
plotTable <- table %>%
  group_by(Prediction) %>%
  mutate(prop = Freq/sum(Freq))
ggplot(data =  plotTable, mapping = aes(x = Reference, y = Prediction, alpha = prop)) +
  geom_tile(aes(fill = Freq), colour = "white") +
  geom_text(aes(label = Freq), vjust = .5, fontface  = "bold", alpha = 1, color="white", size=10) +
  scale_fill_gradient(low = "blue", high = "navyblue") +
  theme_bw() + theme(legend.position = "none")
```

### Visualize Results of Scaled Data

#### Train Set Results

```{r}
set <- trainSet
X1 <- seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 <- seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set <- expand.grid(X1, X2)
colnames(grid_set) <- c('Book.Rating', 'Year.Of.Publication')
y_grid <- knn(train = trainSet[, -3], test = grid_set, cl = trainSet[, 3], k = 5)
plot(set[, -3], main = 'K-NN (Scaled Train Set)',
     xlab = 'Book.Rating', ylab = 'Year.Of.Publication',
     xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'deepskyblue', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'blue3', 'red3'))
```

#### Test Set Results

```{r}
set = testSet
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Book.Rating', 'Year.Of.Publication')
y_grid = knn(train = trainSet[, -3], test = grid_set, cl = trainSet[, 3], k = 5)
plot(set[, -3],
     main = 'K-NN (Scaled Test set)',
     xlab = 'Book.Rating', ylab = 'Year.Of.Publication',
     xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
```

### 8.2.c) KNN is also good at classifying classes.

## b) Compare decision tree model accuracy vs. knn model accuracy

```{r}
print("Accuracy of decision tree")
decision.accuracy.balanced

print("Accuracy of knn")
knn.accuracy
```

As seen above Decision tree has higher accuracy than KNN.

